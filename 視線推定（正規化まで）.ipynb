{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X7tLHvrLnmL",
        "outputId": "b7289ed1-8ffd-4f60-836d-ddc83f3d66c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'data-preprocessing-gaze' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/xucong-zhang/data-preprocessing-gaze.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYb-htq8TXO6"
      },
      "outputs": [],
      "source": [
        "from sys import setdlopenflags\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "class model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model, self).__init__()\n",
        "\n",
        "        efficientnet = torchvision.models.efficientnet_b0( weights='DEFAULT')\n",
        "        self.convNet = efficientnet.features\n",
        "        self.convNet[0]=nn.Conv2d(1,32,3,stride=2)\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(5120, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(1024+2, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 2),\n",
        "        )\n",
        "    def forward(self, x_in):\n",
        "        feature = self.convNet(x_in['eye'])\n",
        "        feature = torch.flatten(feature, start_dim=1)\n",
        "        feature = self.FC(feature)\n",
        "        feature = torch.cat((feature, x_in['head_pose']), 1)\n",
        "        gaze = self.output(feature)\n",
        "        return gaze\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                nn.init.zeros_(m.bias)\n",
        "    def load_model(self,path,device):\n",
        "        self.to(device)\n",
        "        self.load_state_dict(torch.load(path,map_location=device))\n",
        "        self.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEqedSEcLMUl",
        "outputId": "32f7de2a-da40-46a6-ecbd-0768b32eecab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[994.73532636   0.         624.66344095]\n",
            " [  0.         998.16646784 364.08742557]\n",
            " [  0.           0.           1.        ]] [[-0.16321888  0.66783406 -0.00121854 -0.00303158 -1.02159927]]\n",
            "[[[-4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.62995769e+01  6.85950353e+01 -9.86076132e-32]]\n",
            "\n",
            " [[ 2.62995769e+01  6.85950353e+01 -9.86076132e-32]]]\n",
            "Right eye center: [-39.15079064  -0.24188652   1.19851492]\n",
            "Left eye center: [-27.25883569   0.24188652  -1.19851492]\n",
            "Right eye 3D position: [309.17509937  44.75130893 368.64569498]\n",
            "Left eye 3D position: [316.38093984  44.52215028 358.87729028]\n",
            "[[994.73532636   0.         624.66344095]\n",
            " [  0.         998.16646784 364.08742557]\n",
            " [  0.           0.           1.        ]] [[-0.16321888  0.66783406 -0.00121854 -0.00303158 -1.02159927]]\n",
            "[[[-4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.62995769e+01  6.85950353e+01 -9.86076132e-32]]\n",
            "\n",
            " [[ 2.62995769e+01  6.85950353e+01 -9.86076132e-32]]]\n",
            "Right eye center: [-39.15079064  -0.24188652   1.19851492]\n",
            "Left eye center: [-27.25883569   0.24188652  -1.19851492]\n",
            "Right eye 3D position: [-75.33189677  37.05453627 475.07062332]\n",
            "Left eye 3D position: [-63.23381528  37.3794601  476.03458901]\n",
            "[[994.73532636   0.         624.66344095]\n",
            " [  0.         998.16646784 364.08742557]\n",
            " [  0.           0.           1.        ]] [[-0.16321888  0.66783406 -0.00121854 -0.00303158 -1.02159927]]\n",
            "[[[-4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.62995769e+01  6.85950353e+01 -9.86076132e-32]]\n",
            "\n",
            " [[ 2.62995769e+01  6.85950353e+01 -9.86076132e-32]]]\n",
            "Right eye center: [-39.15079064  -0.24188652   1.19851492]\n",
            "Left eye center: [-27.25883569   0.24188652  -1.19851492]\n",
            "Right eye 3D position: [-73.710247   -49.78488603 660.86892239]\n",
            "Left eye 3D position: [-61.58693488 -49.16659421 661.07239115]\n",
            "[[994.73532636   0.         624.66344095]\n",
            " [  0.         998.16646784 364.08742557]\n",
            " [  0.           0.           1.        ]] [[-0.16321888  0.66783406 -0.00121854 -0.00303158 -1.02159927]]\n",
            "[[[-4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.62995769e+01  6.85950353e+01 -9.86076132e-32]]\n",
            "\n",
            " [[ 2.62995769e+01  6.85950353e+01 -9.86076132e-32]]]\n",
            "Right eye center: [-39.15079064  -0.24188652   1.19851492]\n",
            "Left eye center: [-27.25883569   0.24188652  -1.19851492]\n",
            "Right eye 3D position: [170.08557791 -85.57482947 524.15208766]\n",
            "Left eye 3D position: [180.72388202 -85.37593287 518.30526742]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import dlib\n",
        "predictor_path = \"shape_predictor_68_face_landmarks.dat/shape_predictor_68_face_landmarks.dat\"\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(predictor_path)  # Replace with path to shape predictor\n",
        "def get_facial_landmarks(image, detector, predictor):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        return None, None\n",
        "\n",
        "    # Use the largest face if multiple faces are detected\n",
        "    face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
        "\n",
        "    landmarks = predictor(gray, face)\n",
        "    points = np.array([[landmarks.part(n).x, landmarks.part(n).y] for n in range(68)])\n",
        "\n",
        "    # Extract the 2D coordinates of specific landmarks (eyes and mouth corners)\n",
        "    keypoints = points[[36, 39, 42, 45,48,54]]  # Right eye corner, left eye corner, nose tip, right mouth, left mouth, nose bridge\n",
        "    return keypoints, points\n",
        "\n",
        "def draw_gaze(image_in, pitchyaw, thickness=2, color=(0, 0, 255)):\n",
        "    \"\"\"Draw gaze angle on given image with a given eye positions.\"\"\"\n",
        "    image_out = image_in\n",
        "    (h, w) = image_in.shape[:2]\n",
        "    length = np.min([h, w]) / 2.0\n",
        "    pos = (int(w / 2.0), int(h / 2.0))\n",
        "    if len(image_out.shape) == 2 or image_out.shape[2] == 1:  # Convert to RGB if grayscale\n",
        "        image_out = cv2.cvtColor(image_out, cv2.COLOR_GRAY2BGR)\n",
        "    dx = -length * np.sin(pitchyaw[1]) * np.cos(pitchyaw[0])\n",
        "    dy = -length * np.sin(pitchyaw[0])\n",
        "    #視線を書く\n",
        "    cv2.arrowedLine(image_out, tuple(np.round(pos).astype(int)),\n",
        "                  tuple(np.round([pos[0] + dx, pos[1] + dy]).astype(int)), color,\n",
        "                 thickness, cv2.LINE_AA, tipLength=0.2)\n",
        "\n",
        "    return image_out\n",
        "\n",
        "def estimateHeadPose(landmarks, face_model, camera, distortion, iterate=True):\n",
        "    ret, rvec, tvec = cv2.solvePnP(face_model, landmarks, camera, distortion, flags=cv2.SOLVEPNP_EPNP)\n",
        "\n",
        "    if iterate:\n",
        "        ret, rvec, tvec = cv2.solvePnP(face_model, landmarks, camera, distortion, rvec, tvec, True)\n",
        "\n",
        "    return rvec, tvec\n",
        "\n",
        "def normalizeData(img, face, hr, ht, gc, cam):\n",
        "    focal_norm = 960\n",
        "    distance_norm = 600\n",
        "    roiSize = (60, 36)\n",
        "\n",
        "    img_u = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ht = ht.reshape((3, 1))\n",
        "    gc = gc.reshape((3, 1))\n",
        "    hR = cv2.Rodrigues(hr)[0]\n",
        "    Fc = np.dot(hR, face) + ht\n",
        "    re = 0.5 * (Fc[:, 0] + Fc[:, 1]).reshape((3, 1))\n",
        "    le = 0.5 * (Fc[:, 2] + Fc[:, 3]).reshape((3, 1))\n",
        "\n",
        "    data = []\n",
        "    for et in [re, le]:\n",
        "        distance = np.linalg.norm(et)\n",
        "\n",
        "        z_scale = distance_norm / distance\n",
        "        cam_norm = np.array([\n",
        "            [focal_norm, 0, roiSize[0] / 2],\n",
        "            [0, focal_norm, roiSize[1] / 2],\n",
        "            [0, 0, 1.0],\n",
        "        ])\n",
        "        S = np.array([\n",
        "            [1.0, 0.0, 0.0],\n",
        "            [0.0, 1.0, 0.0],\n",
        "            [0.0, 0.0, z_scale],\n",
        "        ])\n",
        "\n",
        "        hRx = hR[:, 0]\n",
        "        forward = (et / distance).reshape(3)\n",
        "        down = np.cross(forward, hRx)\n",
        "        down /= np.linalg.norm(down)\n",
        "        right = np.cross(down, forward)\n",
        "        right /= np.linalg.norm(right)\n",
        "        R = np.c_[right, down, forward].T\n",
        "\n",
        "        W = np.dot(np.dot(cam_norm, S), np.dot(R, np.linalg.inv(cam)))\n",
        "\n",
        "        img_warped = cv2.warpPerspective(img_u, W, roiSize)\n",
        "        img_warped = cv2.equalizeHist(img_warped)\n",
        "\n",
        "        hR_norm = np.dot(R, hR)\n",
        "        hr_norm = cv2.Rodrigues(hR_norm)[0]\n",
        "\n",
        "        gc_normalized = gc - et\n",
        "        gc_normalized = np.dot(R, gc_normalized)\n",
        "        gc_normalized = gc_normalized / np.linalg.norm(gc_normalized)\n",
        "\n",
        "        data.append([img_warped, hr_norm, gc_normalized])\n",
        "\n",
        "    return data\n",
        "\n",
        "def fetch_eyes(path):\n",
        "    fid = cv2.FileStorage('data-preprocessing-gaze/data/calibration/cameraCalib.xml', cv2.FileStorage_READ)\n",
        "    camera_matrix = fid.getNode(\"camera_matrix\").mat()\n",
        "    camera_distortion = fid.getNode(\"cam_distortion\").mat()\n",
        "    print(camera_matrix,camera_distortion)\n",
        "    filepath = os.path.join(path)\n",
        "    img_original = cv2.imread(filepath)\n",
        "    img = cv2.undistort(img_original, camera_matrix, camera_distortion)\n",
        "\n",
        "    # Assuming detector and predictor have been loaded from Dlib\n",
        "    landmarks, general_landmarks = get_facial_landmarks(img, detector, predictor)\n",
        "\n",
        "    face = np.loadtxt('data-preprocessing-gaze/data/faceModelGeneric.txt')\n",
        "    num_pts = face.shape[1]\n",
        "    facePts = face.T.reshape(num_pts, 1, 3)\n",
        "    print(facePts)\n",
        "    landmarks = landmarks.astype(np.float32)\n",
        "    landmarks = landmarks.reshape(num_pts, 1, 2)\n",
        "    hr, ht = estimateHeadPose(landmarks, facePts, camera_matrix, camera_distortion)\n",
        "\n",
        "    gc = np.array([-127.790719, 4.621111, -12.025310])\n",
        "\n",
        "    data = normalizeData(img, face, hr, ht, gc, camera_matrix)\n",
        "\n",
        "    gaze_left = data[1][2]\n",
        "    gaze_right = data[0][2]\n",
        "    lr = [\"right\", \"left\"]\n",
        "\n",
        "    def write_normalized(num):\n",
        "        gaze_direction = data[num][2]\n",
        "        gaze_theta = np.arcsin((-1) * gaze_direction[1])\n",
        "        gaze_phi = np.arctan2((-1) * gaze_direction[0], (-1) * gaze_direction[2])\n",
        "\n",
        "        img_normalized = data[num][0]\n",
        "        cv2.imwrite(f'img_normalized_{lr[num]}({os.path.basename(path)}).jpg', img_normalized)\n",
        "        # 視線を描く\n",
        "        img_normalized = draw_gaze(img_normalized, np.array([gaze_theta[0], gaze_phi[0]]))\n",
        "\n",
        "    write_normalized(0)\n",
        "    write_normalized(1)\n",
        "\n",
        "    # 両目の中心を計算\n",
        "    right_eye_center = np.mean([facePts[0],facePts[0],facePts[0],facePts[1]],axis=0)[0]\n",
        "    left_eye_center = np.mean([facePts[1],facePts[1],facePts[1],facePts[0]],axis=0)[0]\n",
        "    print(\"Right eye center:\", right_eye_center)\n",
        "    print(\"Left eye center:\", left_eye_center)\n",
        "\n",
        "    return data, hr, ht, general_landmarks, camera_matrix,camera_distortion\n",
        "\n",
        "if __name__==\"__main__\":\n",
        " #好きに変えてください\n",
        "  for dir in os.listdir(\"/content/drive/MyDrive/Face_sample\"):\n",
        "    if dir.endswith(\".jpg\"):\n",
        "      fetch_eyes(os.path.join(\"/content/drive/MyDrive/Face_sample\",dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyAnZLMsFdtS",
        "outputId": "c8a7481b-651f-4fa1-d7f8-b37ba28e79c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install loguru"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2TSp_HEg6Nmp",
        "outputId": "d05e91d7-6499-4630-c16c-b03fa52a557e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-65-4c91c8706db7>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(path,map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[994.73532636   0.         624.66344095]\n",
            " [  0.         998.16646784 364.08742557]\n",
            " [  0.           0.           1.        ]] [[-0.16321888  0.66783406 -0.00121854 -0.00303158 -1.02159927]]\n",
            "[[[-4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 2.13128582e+01  4.83773045e-01 -2.39702984e+00]]\n",
            "\n",
            " [[ 4.50967681e+01 -4.83773045e-01  2.39702984e+00]]\n",
            "\n",
            " [[-2.62995769e+01  6.85950353e+01 -9.86076132e-32]]\n",
            "\n",
            " [[ 2.62995769e+01  6.85950353e+01 -9.86076132e-32]]]\n",
            "Right eye center: [-39.15079064  -0.24188652   1.19851492]\n",
            "Left eye center: [-27.25883569   0.24188652  -1.19851492]\n",
            "Right eye 3D position: [-73.710247   -49.78488603 660.86892239]\n",
            "Left eye 3D position: [-61.58693488 -49.16659421 661.07239115]\n",
            "img_right tensor([[[[ 90.,  94.,  94.,  ..., 154., 148., 148.]],\n",
            "\n",
            "         [[ 90.,  90.,  90.,  ..., 161., 154., 154.]],\n",
            "\n",
            "         [[ 90.,  90.,  90.,  ..., 170., 170., 170.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[224., 224., 216.,  ..., 242., 224., 207.]],\n",
            "\n",
            "         [[232., 232., 224.,  ..., 245., 237., 224.]],\n",
            "\n",
            "         [[232., 232., 232.,  ..., 245., 237., 232.]]]])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 36, 1, 60] to have 1 channels, but got 36 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-199d050a2c67>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Execute get_spot on a sample image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Face_sample/down_left.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(tensor([5604.6494,  -29.9152], dtype=torch.float64), tensor([5919.3479, -601.9444], dtype=torch.float64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Face_sample/down_right.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(tensor([2993.7550,  767.3470], dtype=torch.float64), tensor([17490.5185,  -998.2346], dtype=torch.float64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Face_sample/top_left.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(tensor([10789.9999,   627.1514], dtype=torch.float64), tensor([15016.3676, -1983.7483], dtype=torch.float64))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-199d050a2c67>\u001b[0m in \u001b[0;36mget_spot\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Predict gaze for left and right eye\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mgaze_left_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meye_left\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The model returns a tensor of shape [2], [yaw, pitch]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mgaze_right_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meye_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-4c91c8706db7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 3, 3], expected input[1, 36, 1, 60] to have 1 channels, but got 36 channels instead"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from cv2 import Rodrigues\n",
        "from loguru import logger\n",
        "\n",
        "def to2d_gaze(gaze):\n",
        "    yaw = np.degrees(np.arctan2(-gaze[0], -gaze[2]))\n",
        "    pitch = np.degrees(np.arcsin(-gaze[1]))\n",
        "    return [yaw, pitch]\n",
        "\n",
        "def to2d_head(head_pose):\n",
        "    head_pose_rotated = Rodrigues(head_pose)[0][-1]\n",
        "    yaw = np.degrees(np.arctan2(head_pose_rotated[0], head_pose_rotated[2]))\n",
        "    pitch = -np.degrees(np.arcsin(head_pose_rotated[1]))\n",
        "    return [yaw, pitch]\n",
        "\n",
        "def angles_to_vector(yaw, pitch):\n",
        "    \"\"\"\n",
        "    Convert yaw and pitch angles to a gaze vector.\n",
        "\n",
        "    Parameters:\n",
        "    yaw (float): Yaw angle (in degrees)\n",
        "    pitch (float): Pitch angle (in degrees)\n",
        "\n",
        "    Returns:\n",
        "    torch.tensor: Gaze vector (x, y, z)\n",
        "    \"\"\"\n",
        "    # Convert degrees to radians for trigonometric functions\n",
        "    yaw = np.radians(yaw)\n",
        "    pitch = np.radians(pitch)\n",
        "\n",
        "    # Calculate gaze vector components\n",
        "    x = np.cos(pitch) * np.cos(yaw)\n",
        "    y = np.cos(pitch) * np.sin(yaw)\n",
        "    z = np.sin(pitch)\n",
        "\n",
        "    return torch.tensor([x, y, z])\n",
        "\n",
        "def calculate_gaze_point(eye_position, gaze_vector):\n",
        "    \"\"\"\n",
        "    Calculate the intersection of gaze vector with xy-plane (z=0).\n",
        "\n",
        "    Parameters:\n",
        "    eye_position (torch.tensor): Eye position (x, y, z)\n",
        "    gaze_vector (torch.tensor): Gaze vector (x, y, z)\n",
        "\n",
        "    Returns:\n",
        "    torch.tensor: Intersection point on xy-plane (x, y)\n",
        "    \"\"\"\n",
        "    if gaze_vector[2] == 0:\n",
        "        return None  # Parallel to xy-plane, no intersection\n",
        "\n",
        "    t = -eye_position[2] / gaze_vector[2]  # Solve for z=0\n",
        "    gaze_x = eye_position[0] + t * gaze_vector[0]\n",
        "    gaze_y = eye_position[1] + t * gaze_vector[1]\n",
        "\n",
        "    return torch.tensor([gaze_x, gaze_y])\n",
        "\n",
        "def get_spot(image_path):\n",
        "    # Load camera calibration data\n",
        "    fid = cv2.FileStorage('data-preprocessing-gaze/data/calibration/cameraCalib.xml', cv2.FileStorage_READ)\n",
        "    camera_matrix = fid.getNode(\"camera_matrix\").mat()\n",
        "    camera_distortion = fid.getNode(\"cam_distortion\").mat()\n",
        "\n",
        "    # Load and undistort the image\n",
        "    img_original = cv2.imread(image_path)\n",
        "    img = cv2.undistort(img_original, camera_matrix, camera_distortion)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load model and process eyes\n",
        "    net = model()\n",
        "    net.load_model(\"GazeNet/Iter_1_GazeNet.pt\", device)\n",
        "\n",
        "    data, hr, ht, landmarks, right_eye_3D, left_eye_3D = fetch_eyes(image_path)\n",
        "    img_right = torch.from_numpy(data[0][0]).unsqueeze(0).unsqueeze(2).to(device).type(torch.FloatTensor)\n",
        "    img_left = torch.from_numpy(data[1][0]).unsqueeze(0).unsqueeze(2).to(device).type(torch.FloatTensor)\n",
        "    print(\"img_right\",img_right)\n",
        "    # Prepare eye and head pose data for inference\n",
        "    eye_left = {\"eye\": img_left, \"head_pose\": torch.tensor([to2d_head(hr)]).to(device).type(torch.FloatTensor)}\n",
        "    eye_right = {\"eye\": img_right, \"head_pose\": torch.tensor([to2d_head(hr)]).to(device).type(torch.FloatTensor)}\n",
        "\n",
        "    # Predict gaze for left and right eye\n",
        "    gaze_left_output = net(eye_left)  # The model returns a tensor of shape [2], [yaw, pitch]\n",
        "    gaze_right_output = net(eye_right)\n",
        "\n",
        "    # Split the output tensor into yaw and pitch (convert tensor to scalars for angles_to_vector)\n",
        "    yaw_left, pitch_left = gaze_left_output[0][0].item(), gaze_left_output[0][1].item()\n",
        "    yaw_right, pitch_right = gaze_right_output[0][0].item(), gaze_right_output[0][1].item()\n",
        "    print(\"gaze_left:\",yaw_left,pitch_left)\n",
        "    print(\"gaze_right:\",yaw_right,pitch_right)\n",
        "    # Convert yaw and pitch to gaze vectors\n",
        "    gaze_left = angles_to_vector(yaw_left, pitch_left)\n",
        "    gaze_right = angles_to_vector(yaw_right, pitch_right)\n",
        "\n",
        "    # Calculate eye points using projection\n",
        "    objectPoints = np.array([left_eye_3D, right_eye_3D]).reshape(-1, 1, 3)\n",
        "    print(objectPoints)\n",
        "    left_eye_point, right_eye_point = cv2.projectPoints(objectPoints, hr, ht, camera_matrix, camera_distortion)\n",
        "    print(gaze_left,gaze_right)\n",
        "    # Calculate gaze points\n",
        "    left_point = calculate_gaze_point(left_eye_3D, gaze_left)\n",
        "    right_point = calculate_gaze_point(right_eye_3D, gaze_right)\n",
        "\n",
        "    return left_point, right_point\n",
        "\n",
        "# Execute get_spot on a sample image\n",
        "logger.warning(get_spot(\"/content/drive/MyDrive/Face_sample/down_left.jpg\"))#(tensor([5604.6494,  -29.9152], dtype=torch.float64), tensor([5919.3479, -601.9444], dtype=torch.float64))\n",
        "logger.warning(get_spot(\"/content/drive/MyDrive/Face_sample/down_right.jpg\"))#(tensor([2993.7550,  767.3470], dtype=torch.float64), tensor([17490.5185,  -998.2346], dtype=torch.float64))\n",
        "logger.warning(get_spot(\"/content/drive/MyDrive/Face_sample/top_left.jpg\"))#(tensor([10789.9999,   627.1514], dtype=torch.float64), tensor([15016.3676, -1983.7483], dtype=torch.float64))\n",
        "logger.warning(get_spot(\"/content/drive/MyDrive/Face_sample/top_right.jpg\"))#(tensor([11557.7793,  3258.8042], dtype=torch.float64), tensor([19544.6680,  2486.4446], dtype=torch.float64))\n",
        "logger.warning(get_spot(\"/content/WIN_20241001_16_54_37_Pro.jpg\"))#(tensor([2715.8031,  224.1377], dtype=torch.float64), tensor([2597.8703, -316.0890], dtype=torch.float64))\n",
        "logger.warning(get_spot(\"/content/WIN_20241001_18_18_13_Pro.jpg\"))\n",
        "logger.warning(get_spot(\"/content/WIN_20241001_18_18_23_Pro.jpg\"))\n",
        "logger.warning(get_spot(\"/content/WIN_20241001_18_45_36_Pro.jpg\"))\n",
        "logger.warning(get_spot(\"/content/WIN_20241001_19_01_43_Pro.jpg\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
